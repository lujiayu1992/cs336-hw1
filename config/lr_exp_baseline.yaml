model:
  d_model: 512
  num_heads: 16
  d_ff: 1344
  num_layers: 4
  vocab_size: 10000
  context_length: 256
  rope_theta: 10000.0

optimizer:
  max_learning_rate: 0.001
  min_learning_rate: 0.0001
  warmup_iters: 100
  cosine_iters: 10000
  weight_decay: 0.01
  max_l2_norm: 0.2

training:
  batch_size: 32
  max_iters: 10000
  log_every: 100
  val_every: 500
  checkpoint_every: 100
  checkpoint_path: "checkpoints/exp_run/lr_baseline_model.pt"
  device: "auto"
  wandb: true
  wandb_project: "experiment"
  wandb_run: "lr_baseline"

dataset:
  train_id_path: "data/train_id.npy"
  valid_id_path: "data/valid_id.npy"
  train_path: "data/TinyStoriesV2-GPT4-train.txt"
  valid_path: "data/TinyStoriesV2-GPT4-valid.txt"
  vocab_out: "output/tinystories_vocab.pkl"
  merges_out: "output/tinystories_merges.pkl"
  special_tokens: ["<PAD>", "<UNK>", "<BOS>", "<EOS>", "<|endoftext|>"]

generation:
  checkpoint_path: "checkpoints/model.pt" # New: Specific checkpoint to load for generation
  prompt: "Once upon a time, in a land full of candy," # New: Default prompt for generation
  max_new_tokens: 150 # New: Default length of generated text
  temperature: 0.8 # New: Sampling temperature
  top_k: 200 # New: Top-k sampling
